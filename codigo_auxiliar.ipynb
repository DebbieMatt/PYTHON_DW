{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5SXUVmSAa6FZDr8EIte4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebbieMatt/PYTHON_DW/blob/main/codigo_auxiliar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Data Warehouse IBGE - Pipeline ETL Melhorado\n",
        "Autor: D√©bora Mateus\n",
        "Descri√ß√£o: Sistema completo de ETL para dados do IBGE com valida√ß√£o,\n",
        "           logging e tratamento de erros robusto\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "22LkQV5N-v2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3 as sql\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from typing import Optional, Dict, Tuple\n",
        "import sys"
      ],
      "metadata": {
        "id": "-_00lCAQ_5cq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CONFIGURA√á√ÉO DE LOGGING ====================\n",
        "def configurar_logging(nivel=logging.INFO):\n",
        "    \"\"\"\n",
        "    Configura sistema de logging para rastreamento do pipeline\n",
        "    \"\"\"\n",
        "    logging.basicConfig(\n",
        "        level=nivel,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        handlers=[\n",
        "            logging.StreamHandler(sys.stdout),\n",
        "            logging.FileHandler('etl_pipeline.log', encoding='utf-8')\n",
        "        ]\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "logger = configurar_logging()"
      ],
      "metadata": {
        "id": "7bh4BN3RAFcZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CONFIGURA√á√ïES GLOBAIS ====================\n",
        "class ConfigETL:\n",
        "    \"\"\"Classe de configura√ß√£o centralizada\"\"\"\n",
        "\n",
        "    # Caminhos\n",
        "    BASE_DIR = Path('/content/drive/MyDrive/Colab Notebooks')\n",
        "    BDODS_PATH = BASE_DIR / \"ODS.db\"\n",
        "    BDDW_PATH = BASE_DIR / \"DW.db\"\n",
        "\n",
        "    # URLs\n",
        "    URL_IBGE_MUNICIPIOS = 'https://www.ibge.gov.br/explica/codigos-dos-municipios.php#MT'\n",
        "\n",
        "    # Arquivos CSV\n",
        "    CSV_PIB_SETOR = \"PIB-SETOR-MT.csv\"\n",
        "    CSV_PIB_MT = \"PIB-MT.csv\"\n",
        "\n",
        "    # Nomes de tabelas\n",
        "    TABELA_LOG_MUNIC = \"tabelaLogMunic\"\n",
        "    TABELA_DIM_MUNIC = \"dMunicipio\"\n",
        "    TABELA_LOG_PIB_SETOR = \"tbLogPIBsetor\"\n",
        "    TABELA_LOG_PIB_MT = \"tbLogPIBmt\"\n",
        "\n",
        "    @staticmethod\n",
        "    def obter_timestamp():\n",
        "        \"\"\"Retorna timestamp formatado para carga\"\"\"\n",
        "        return datetime.today().strftime('%d/%m/%Y %H:%M:%S')"
      ],
      "metadata": {
        "id": "8Tz-Xs8bAKEM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== EXTRA√á√ÉO DE DADOS ====================\n",
        "class ExtractorIBGE:\n",
        "    \"\"\"Classe respons√°vel pela extra√ß√£o de dados do IBGE\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extrair_municipios(url: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extrai dados de munic√≠pios do IBGE via web scraping\n",
        "\n",
        "        Args:\n",
        "            url: URL da p√°gina do IBGE\n",
        "\n",
        "        Returns:\n",
        "            DataFrame com dados dos munic√≠pios ou None em caso de erro\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Iniciando extra√ß√£o de munic√≠pios: {url}\")\n",
        "\n",
        "            df_municipios = pd.read_html(url, match=\"Munic√≠pios de Mato Grosso\")[0]\n",
        "\n",
        "            # Renomear colunas\n",
        "            df_municipios = df_municipios.rename(columns={\n",
        "                'Munic√≠pios de Mato Grosso': 'Munic',\n",
        "                'C√≥digos': 'Cod'\n",
        "            })\n",
        "\n",
        "            # Configurar √≠ndice\n",
        "            df_municipios.index.name = 'ID'\n",
        "            df_municipios.index = df_municipios.index + 1\n",
        "\n",
        "            # Adicionar timestamp de carga\n",
        "            df_municipios['dataCarga'] = ConfigETL.obter_timestamp()\n",
        "\n",
        "            logger.info(f\"‚úì Extra√≠dos {len(df_municipios)} munic√≠pios com sucesso\")\n",
        "            return df_municipios\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao extrair munic√≠pios: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def extrair_csv(caminho_arquivo: str) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Extrai dados de arquivo CSV\n",
        "\n",
        "        Args:\n",
        "            caminho_arquivo: Caminho completo do arquivo CSV\n",
        "\n",
        "        Returns:\n",
        "            DataFrame com dados do CSV ou None em caso de erro\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Iniciando leitura do CSV: {caminho_arquivo}\")\n",
        "\n",
        "            if not Path(caminho_arquivo).exists():\n",
        "                raise FileNotFoundError(f\"Arquivo n√£o encontrado: {caminho_arquivo}\")\n",
        "\n",
        "            df = pd.read_csv(caminho_arquivo, encoding='utf-8')\n",
        "            df['dtCarga'] = ConfigETL.obter_timestamp()\n",
        "\n",
        "            logger.info(f\"‚úì CSV carregado: {len(df)} registros\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao ler CSV: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "7_FS4BqvAOvJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== TRANSFORMA√á√ÉO DE DADOS ====================\n",
        "class TransformadorDados:\n",
        "    \"\"\"Classe respons√°vel pela transforma√ß√£o e valida√ß√£o de dados\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validar_dataframe(df: pd.DataFrame, colunas_obrigatorias: list) -> bool:\n",
        "        \"\"\"\n",
        "        Valida se DataFrame possui todas as colunas obrigat√≥rias\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame a ser validado\n",
        "            colunas_obrigatorias: Lista de colunas que devem existir\n",
        "\n",
        "        Returns:\n",
        "            True se v√°lido, False caso contr√°rio\n",
        "        \"\"\"\n",
        "        colunas_faltantes = set(colunas_obrigatorias) - set(df.columns)\n",
        "\n",
        "        if colunas_faltantes:\n",
        "            logger.error(f\"Colunas obrigat√≥rias faltantes: {colunas_faltantes}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def limpar_dados_municipios(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Limpa e valida dados de munic√≠pios\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com dados brutos\n",
        "\n",
        "        Returns:\n",
        "            DataFrame limpo\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Remover valores nulos\n",
        "            df = df.dropna(subset=['Munic', 'Cod'])\n",
        "\n",
        "            # Normalizar nomes de munic√≠pios\n",
        "            df['Munic'] = df['Munic'].str.strip().str.title()\n",
        "\n",
        "            # Validar c√≥digos IBGE (devem ter 7 d√≠gitos)\n",
        "            df['Cod'] = df['Cod'].astype(str).str.zfill(7)\n",
        "\n",
        "            # Remover duplicatas\n",
        "            df = df.drop_duplicates(subset=['Cod'])\n",
        "\n",
        "            logger.info(f\"‚úì Dados de munic√≠pios limpos: {len(df)} registros v√°lidos\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao limpar dados: {str(e)}\")\n",
        "            return df\n",
        "\n",
        "    @staticmethod\n",
        "    def preparar_dimensao(df: pd.DataFrame, colunas: list) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Prepara dados para carregamento na dimens√£o\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame original\n",
        "            colunas: Lista de colunas a manter\n",
        "\n",
        "        Returns:\n",
        "            DataFrame preparado para dimens√£o\n",
        "        \"\"\"\n",
        "        return df[colunas].copy()"
      ],
      "metadata": {
        "id": "1qFIvDh_AUGt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== CARGA DE DADOS ====================\n",
        "class LoaderDW:\n",
        "    \"\"\"Classe respons√°vel pela carga de dados nos bancos\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def criar_bancos() -> Tuple[bool, bool]:\n",
        "        \"\"\"\n",
        "        Cria bancos de dados ODS e DW se n√£o existirem\n",
        "\n",
        "        Returns:\n",
        "            Tupla (ods_criado, dw_criado)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not ConfigETL.BASE_DIR.exists():\n",
        "                logger.error(f\"‚úó Diret√≥rio base n√£o existe: {ConfigETL.BASE_DIR}\")\n",
        "                return False, False\n",
        "\n",
        "            ods_existe = ConfigETL.BDODS_PATH.exists()\n",
        "            dw_existe = ConfigETL.BDDW_PATH.exists()\n",
        "\n",
        "            if not ods_existe:\n",
        "                ConfigETL.BDODS_PATH.touch()\n",
        "                logger.info(\"‚úì Banco ODS.db criado\")\n",
        "\n",
        "            if not dw_existe:\n",
        "                ConfigETL.BDDW_PATH.touch()\n",
        "                logger.info(\"‚úì Banco DW.db criado\")\n",
        "\n",
        "            if ods_existe and dw_existe:\n",
        "                logger.info(\"‚úì Bancos de dados j√° existem\")\n",
        "\n",
        "            return True, True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao criar bancos: {str(e)}\")\n",
        "            return False, False\n",
        "\n",
        "    @staticmethod\n",
        "    def carregar_ods(df: pd.DataFrame, tabela: str) -> bool:\n",
        "        \"\"\"\n",
        "        Carrega dados no ODS (Operational Data Store)\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com dados a carregar\n",
        "            tabela: Nome da tabela de destino\n",
        "\n",
        "        Returns:\n",
        "            True se bem-sucedido, False caso contr√°rio\n",
        "        \"\"\"\n",
        "        conexao = None\n",
        "        try:\n",
        "            logger.info(f\"Carregando {len(df)} registros na tabela ODS: {tabela}\")\n",
        "\n",
        "            conexao = sql.connect(ConfigETL.BDODS_PATH)\n",
        "            df.to_sql(tabela, conexao, if_exists=\"append\", index=True)\n",
        "            conexao.commit()\n",
        "\n",
        "            logger.info(f\"‚úì Carga ODS conclu√≠da: {tabela}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao carregar ODS: {str(e)}\")\n",
        "            if conexao:\n",
        "                conexao.rollback()\n",
        "            return False\n",
        "\n",
        "        finally:\n",
        "            if conexao:\n",
        "                conexao.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def carregar_dw(df: pd.DataFrame, tabela: str, if_exists: str = \"replace\") -> bool:\n",
        "        \"\"\"\n",
        "        Carrega dados no Data Warehouse\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame com dados a carregar\n",
        "            tabela: Nome da tabela de destino\n",
        "            if_exists: Comportamento se tabela existir ('replace', 'append', 'fail')\n",
        "\n",
        "        Returns:\n",
        "            True se bem-sucedido, False caso contr√°rio\n",
        "        \"\"\"\n",
        "        conexao = None\n",
        "        try:\n",
        "            logger.info(f\"Carregando {len(df)} registros na tabela DW: {tabela}\")\n",
        "\n",
        "            conexao = sql.connect(ConfigETL.BDDW_PATH)\n",
        "            df.to_sql(tabela, conexao, if_exists=if_exists, index=True)\n",
        "            conexao.commit()\n",
        "\n",
        "            logger.info(f\"‚úì Carga DW conclu√≠da: {tabela}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro ao carregar DW: {str(e)}\")\n",
        "            if conexao:\n",
        "                conexao.rollback()\n",
        "            return False\n",
        "\n",
        "        finally:\n",
        "            if conexao:\n",
        "                conexao.close()"
      ],
      "metadata": {
        "id": "6TtzJabEAYO4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== PIPELINE ETL PRINCIPAL ====================\n",
        "class PipelineETL:\n",
        "    \"\"\"Orquestrador principal do pipeline ETL\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.extractor = ExtractorIBGE()\n",
        "        self.transformer = TransformadorDados()\n",
        "        self.loader = LoaderDW()\n",
        "        self.metricas = {\n",
        "            'inicio': datetime.now(),\n",
        "            'registros_processados': 0,\n",
        "            'erros': 0\n",
        "        }\n",
        "\n",
        "    def executar_pipeline_municipios(self) -> bool:\n",
        "        \"\"\"\n",
        "        Executa pipeline completo para dados de munic√≠pios\n",
        "\n",
        "        Returns:\n",
        "            True se bem-sucedido, False caso contr√°rio\n",
        "        \"\"\"\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"INICIANDO PIPELINE: Munic√≠pios IBGE\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # 1. EXTRACT\n",
        "            df_municipios = self.extractor.extrair_municipios(\n",
        "                ConfigETL.URL_IBGE_MUNICIPIOS\n",
        "            )\n",
        "            if df_municipios is None:\n",
        "                return False\n",
        "\n",
        "            # 2. TRANSFORM\n",
        "            df_limpo = self.transformer.limpar_dados_municipios(df_municipios)\n",
        "            df_dimensao = self.transformer.preparar_dimensao(\n",
        "                df_limpo, ['Munic', 'Cod']\n",
        "            )\n",
        "\n",
        "            # 3. LOAD\n",
        "            # Carregar no ODS (dados completos com log)\n",
        "            sucesso_ods = self.loader.carregar_ods(\n",
        "                df_limpo, ConfigETL.TABELA_LOG_MUNIC\n",
        "            )\n",
        "\n",
        "            # Carregar no DW (apenas dimens√£o)\n",
        "            sucesso_dw = self.loader.carregar_dw(\n",
        "                df_dimensao, ConfigETL.TABELA_DIM_MUNIC\n",
        "            )\n",
        "\n",
        "            self.metricas['registros_processados'] += len(df_municipios)\n",
        "\n",
        "            return sucesso_ods and sucesso_dw\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro no pipeline de munic√≠pios: {str(e)}\")\n",
        "            self.metricas['erros'] += 1\n",
        "            return False\n",
        "\n",
        "    def executar_pipeline_pib(self) -> bool:\n",
        "        \"\"\"\n",
        "        Executa pipeline completo para dados de PIB\n",
        "\n",
        "        Returns:\n",
        "            True se bem-sucedido, False caso contr√°rio\n",
        "        \"\"\"\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"INICIANDO PIPELINE: PIB Mato Grosso\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Pipeline PIB por Setor\n",
        "            caminho_pib_setor = str(ConfigETL.BASE_DIR / ConfigETL.CSV_PIB_SETOR)\n",
        "            df_pib_setor = self.extractor.extrair_csv(caminho_pib_setor)\n",
        "\n",
        "            if df_pib_setor is not None:\n",
        "                sucesso_setor = self.loader.carregar_ods(\n",
        "                    df_pib_setor, ConfigETL.TABELA_LOG_PIB_SETOR\n",
        "                )\n",
        "                self.metricas['registros_processados'] += len(df_pib_setor)\n",
        "            else:\n",
        "                sucesso_setor = False\n",
        "\n",
        "            # Pipeline PIB MT\n",
        "            caminho_pib_mt = str(ConfigETL.BASE_DIR / ConfigETL.CSV_PIB_MT)\n",
        "            df_pib_mt = self.extractor.extrair_csv(caminho_pib_mt)\n",
        "\n",
        "            if df_pib_mt is not None:\n",
        "                sucesso_mt = self.loader.carregar_ods(\n",
        "                    df_pib_mt, ConfigETL.TABELA_LOG_PIB_MT\n",
        "                )\n",
        "                self.metricas['registros_processados'] += len(df_pib_mt)\n",
        "            else:\n",
        "                sucesso_mt = False\n",
        "\n",
        "            return sucesso_setor and sucesso_mt\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚úó Erro no pipeline de PIB: {str(e)}\")\n",
        "            self.metricas['erros'] += 1\n",
        "            return False\n",
        "\n",
        "    def executar_completo(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Executa pipeline ETL completo\n",
        "\n",
        "        Returns:\n",
        "            Dicion√°rio com m√©tricas de execu√ß√£o\n",
        "        \"\"\"\n",
        "        logger.info(\"üöÄ INICIANDO PIPELINE ETL COMPLETO\")\n",
        "        logger.info(f\"Timestamp: {ConfigETL.obter_timestamp()}\")\n",
        "\n",
        "        # Criar bancos se necess√°rio\n",
        "        self.loader.criar_bancos()\n",
        "\n",
        "        # Executar pipelines\n",
        "        sucesso_municipios = self.executar_pipeline_municipios()\n",
        "        sucesso_pib = self.executar_pipeline_pib()\n",
        "\n",
        "        # Calcular m√©tricas finais\n",
        "        self.metricas['fim'] = datetime.now()\n",
        "        self.metricas['duracao'] = (\n",
        "            self.metricas['fim'] - self.metricas['inicio']\n",
        "        ).total_seconds()\n",
        "        self.metricas['sucesso_geral'] = sucesso_municipios and sucesso_pib\n",
        "\n",
        "        # Log final\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"PIPELINE ETL CONCLU√çDO\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(f\"Status: {'‚úì SUCESSO' if self.metricas['sucesso_geral'] else '‚úó FALHA'}\")\n",
        "        logger.info(f\"Registros processados: {self.metricas['registros_processados']}\")\n",
        "        logger.info(f\"Erros encontrados: {self.metricas['erros']}\")\n",
        "        logger.info(f\"Dura√ß√£o: {self.metricas['duracao']:.2f} segundos\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        return self.metricas"
      ],
      "metadata": {
        "id": "gLAjXOUoAcf2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== EXECU√á√ÉO ====================\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Ponto de entrada principal do pipeline\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Inicializar e executar pipeline\n",
        "        pipeline = PipelineETL()\n",
        "        metricas = pipeline.executar_completo()\n",
        "\n",
        "        # Exibir resumo\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"RESUMO DA EXECU√á√ÉO\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"‚úì Registros processados: {metricas['registros_processados']}\")\n",
        "        print(f\"‚úì Tempo total: {metricas['duracao']:.2f}s\")\n",
        "        print(f\"‚úì Status: {'SUCESSO' if metricas['sucesso_geral'] else 'FALHA'}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.warning(\"‚ö† Pipeline interrompido pelo usu√°rio\")\n",
        "        sys.exit(1)\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"‚úó Erro cr√≠tico no pipeline: {str(e)}\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iojQtDZoAifv",
        "outputId": "682110a3-206c-4a7a-f329-1fa913376b2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RESUMO DA EXECU√á√ÉO\n",
            "============================================================\n",
            "‚úì Registros processados: 426\n",
            "‚úì Tempo total: 1.97s\n",
            "‚úì Status: SUCESSO\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}